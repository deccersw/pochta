# –ë–õ–û–ö 1: –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import umap.umap_ as umap
import hdbscan
from tqdm import tqdm
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from collections import Counter
from datetime import datetime
import warnings
import spacy
import os

warnings.filterwarnings('ignore')

# –ë–õ–û–ö 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("=== –ë–õ–û–ö 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===")

# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É
file_path = "/Users/danial2006/–•–∞–∫–∞—Ç–æ–Ω/–ü–∏—à—É —Ç–µ–±–µ. –ö–æ—Ä–ø—É—Å –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞ (2024).xlsx"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

if not os.path.exists(file_path):
    raise FileNotFoundError(f"–§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–∫–∞–∂–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å.")

df = pd.read_excel(file_path, sheet_name=0)

print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)} –∑–∞–ø–∏—Å–µ–π")

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
df = df[df["–Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–∫—Ä—ã—Ç–∫–∏"] == "—Ä—É—Å—Å–∫–∏–π"].dropna(subset=["–¢–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã—Ç–∫–∏"]).reset_index(drop=True)
print(f" –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: {len(df)} –∑–∞–ø–∏—Å–µ–π")

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—ã –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
print("\nüîß –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—ã...")

# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å—Ç–æ–ª–±—Ü–∞ —Å –¥–∞—Ç–∞–º–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
df['date_processed'] = pd.to_datetime(df['–î–∞—Ç–∞ –æ—Ç–∫—Ä—ã—Ç–∫–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è)'], errors='coerce')

# –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∞—Ç–∞–º
valid_dates = df['date_processed'].notna()
print(f"–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç: {valid_dates.sum()}")
print(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö/–ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞—Ç: {len(df) - valid_dates.sum()}")

if valid_dates.any():
    min_date = df.loc[valid_dates, 'date_processed'].min()
    max_date = df.loc[valid_dates, 'date_processed'].max()
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {min_date} - {max_date}")
else:
    print("–ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

# –î–æ–±–∞–≤–ª—è–µ–º –≥–æ–¥ –∏ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç
df['year'] = df['date_processed'].dt.year
df['decade'] = (df['year'] // 10) * 10

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º
decade_counts = df[df['decade'].notna()]['decade'].value_counts().sort_index()
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è–º:")
for decade, count in decade_counts.items():
    print(f"   {int(decade)}-–µ: {count} –∑–∞–ø–∏—Å–µ–π")

print(f"\n–ì–æ—Ç–æ–≤–æ –∫ –∞–Ω–∞–ª–∏–∑—É: {len(df)} —Ä—É—Å—Å–∫–∏—Ö –æ—Ç–∫—Ä—ã—Ç–æ–∫")

print(df[['–¢–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã—Ç–∫–∏', 'date_processed', 'year', 'decade']].head(3))

# –ë–õ–û–ö 3: –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–û–í
print("\n=== –ë–õ–û–ö 3: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ ===")

def normalize_text(text):
    """–ú—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # –ó–∞–º–µ–Ω–∞ –¥–æ—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö –±—É–∫–≤
    text = re.sub("—£", "–µ", text)
    text = re.sub("—ñ", "–∏", text)
    text = re.sub("—≥", "—Ñ", text)
    text = re.sub("—ä(?=\s|$)", "", text)
    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub("[^–∞-—è—ë\s]", " ", text)
    text = re.sub("\s+", " ", text).strip()
    return text

# –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
df["text_clean"] = df["–¢–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã—Ç–∫–∏"].apply(normalize_text)

print("–ü—Ä–∏–º–µ—Ä—ã –æ—á–∏—â–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤:")
for i in range(2):
    original = df["–¢–µ–∫—Å—Ç –æ—Ç–∫—Ä—ã—Ç–∫–∏"].iloc[i][:100] + "..."
    cleaned = df["text_clean"].iloc[i][:100] + "..."
    print(f"\n–ü—Ä–∏–º–µ—Ä {i+1}:")
    print(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {original}")
    print(f"–û—á–∏—â–µ–Ω–Ω—ã–π: {cleaned}")

print(f"\n–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df)} —Ç–µ–∫—Å—Ç–æ–≤")

# –ë–õ–û–ö 4: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï N-–ì–†–ê–ú–ú
print("\n=== –ë–õ–û–ö 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ n-–≥—Ä–∞–º–º ===")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º CountVectorizer –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–∏–≥—Ä–∞–º–º –∏ —Ç—Ä–∏–≥—Ä–∞–º–º
vectorizer = CountVectorizer(
    analyzer="word",
    ngram_range=(2, 4),  # –ë–∏–≥—Ä–∞–º–º—ã, —Ç—Ä–∏–≥—Ä–∞–º–º—ã –∏ —á–µ—Ç—ã—Ä–µ—Ö–≥—Ä–∞–º–º—ã
    min_df=2,  # –§—Ä–∞–∑–∞ –¥–æ–ª–∂–Ω–∞ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –≤ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    max_df=0.8,  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Ñ—Ä–∞–∑—ã
    token_pattern=r"(?u)\b[–∞-—è—ë]{2,}\b"
)

# –û–±—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–π–∑–µ—Ä –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
X = vectorizer.fit_transform(df["text_clean"])
phrases = vectorizer.get_feature_names_out()
frequencies = X.sum(axis=0).A1

print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(phrases)} n-–≥—Ä–∞–º–º")

# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ñ—Ä–∞–∑–∞–º–∏ –∏ —á–∞—Å—Ç–æ—Ç–∞–º–∏
phrases_df = pd.DataFrame({
    "phrase": phrases,
    "frequency": frequencies
}).sort_values("frequency", ascending=False)

print("\nüèÜ –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ñ—Ä–∞–∑:")
print(phrases_df.head(20))

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
phrase_doc_matrix = X

# –ë–õ–û–ö 5: POS-–§–ò–õ–¨–¢–†–ê–¶–ò–Ø N-–ì–†–ê–ú–ú –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú SPACY
print("\n=== –ë–õ–û–ö 5: POS-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è n-–≥—Ä–∞–º–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º spaCy ===")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ spaCy
try:
    nlp = spacy.load("ru_core_news_md")
    print("–ú–æ–¥–µ–ª—å spaCy –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
except OSError:
    print("–ú–æ–¥–µ–ª—å 'ru_core_news_md' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å –≤—ã–ø–æ–ª–Ω–∏–≤ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:")
    print("python -m spacy download ru_core_news_md")
    exit(1)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è POS-–ø–∞—Ç—Ç–µ—Ä–Ω–∞
def get_pos_pattern(phrase):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å POS-—Ç–µ–≥–∞–º–∏ –¥–ª—è —Ñ—Ä–∞–∑—ã"""
    doc = nlp(phrase)
    return " ".join([t.pos_ for t in doc if not t.is_punct])

print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç–∏ —Ä–µ—á–∏ –¥–ª—è n-–≥—Ä–∞–º–º...")

# –ü–æ–ª—É—á–∞–µ–º POS-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–∑
pos_patterns = []
for phrase in tqdm(phrases):
    pos_patterns.append(get_pos_pattern(phrase))

# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ñ—Ä–∞–∑–∞–º–∏, —á–∞—Å—Ç–æ—Ç–∞–º–∏ –∏ POS-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
df_phr = pd.DataFrame({
    "phrase": phrases, 
    "freq": frequencies, 
    "pattern": pos_patterns
})

print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(df_phr)} —Ñ—Ä–∞–∑")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∞–±–ª–æ–Ω—ã –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑ –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–π
keep_patterns = [
    "VERB NOUN",           # "–ø–æ–∑–¥—Ä–∞–≤–ª—è—é —Å –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–º"
    "VERB ADP NOUN",       # "–∂–µ–ª–∞—é –≤ –Ω–æ–≤—ã–π –≥–æ–¥" 
    "ADJ NOUN",            # "–¥–æ—Ä–æ–≥–æ–π –¥—Ä—É–≥", "—Å—á–∞—Å—Ç–ª–∏–≤–æ–≥–æ —Ä–æ–∂–¥–µ—Å—Ç–≤–∞"
    "PRON ADJ NOUN",       # "–º–æ–π –¥–æ—Ä–æ–≥–æ–π –¥—Ä—É–≥"
    "NOUN NOUN",           # "–¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è"
    "ADV VERB",            # "–∫—Ä–µ–ø–∫–æ —Ü–µ–ª—É—é"
    "VERB ADV",            # "–∂–µ–ª–∞—é —Å–∏–ª—å–Ω–æ"
    "ADJ ADJ NOUN",        # "–¥–æ—Ä–æ–≥–æ–π –º–∏–ª—ã–π –¥—Ä—É–≥"
    "VERB PRON NOUN",      # "–∂–µ–ª–∞—é —Ç–µ–±–µ —Å—á–∞—Å—Ç—å—è"
    "ADP NOUN NOUN",       # "—Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è"
    "VERB DET NOUN",       # "–∂–µ–ª–∞—é –≤—Å–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ"
    "ADJ VERB",            # "—Ä–∞–¥ –ø–æ–∑–¥—Ä–∞–≤–∏—Ç—å"
    "NOUN VERB",           # "–ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–Ω–∏–º–∞—é"
]

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ—Ä–∞–∑—ã –ø–æ —à–∞–±–ª–æ–Ω–∞–º
df_phr_filtered = df_phr[df_phr["pattern"].isin(keep_patterns)].copy()

print(f"–î–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_phr)} —Ñ—Ä–∞–∑")
print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_phr_filtered)} —Ñ—Ä–∞–∑")

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
pattern_stats = df_phr_filtered['pattern'].value_counts()
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ POS-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º:")
for pattern, count in pattern_stats.items():
    print(f"   {pattern}: {count} —Ñ—Ä–∞–∑")

# –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞—à–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
meaningful_phrases = df_phr_filtered['phrase'].tolist()
meaningful_frequencies = df_phr_filtered['freq'].tolist()
meaningful_phrases_df = df_phr_filtered.sort_values('freq', ascending=False)

print("\n–¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ñ—Ä–∞–∑ –ø–æ—Å–ª–µ POS-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
print(meaningful_phrases_df.head(20))

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
print("\n–ü—Ä–∏–º–µ—Ä—ã —Ñ—Ä–∞–∑ –ø–æ –∫–∞–∂–¥–æ–º—É POS-–ø–∞—Ç—Ç–µ—Ä–Ω—É:")
for pattern in keep_patterns:
    examples = df_phr_filtered[df_phr_filtered['pattern'] == pattern].head(2)
    if len(examples) > 0:
        print(f"\n   {pattern}:")
        for _, row in examples.iterrows():
            print(f"      '{row['phrase']}' (—á–∞—Å—Ç–æ—Ç–∞: {row['freq']})")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
print("\n–°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")

fig_patterns = px.bar(
    pattern_stats.reset_index(),
    x='pattern',
    y='count',
    title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∑ –ø–æ POS-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º',
    labels={'pattern': 'POS-–ø–∞—Ç—Ç–µ—Ä–Ω', 'count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑'},
    color='count',
    color_continuous_scale='Viridis'
)

fig_patterns.update_layout(
    xaxis_tickangle=-45,
    height=500
)
fig_patterns.show()

# –°–æ–∑–¥–∞–µ–º treemap –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ—Ä–∞–∑
if len(df_phr_filtered) > 0:
    fig_treemap = px.treemap(
        df_phr_filtered.nlargest(50, 'freq'),
        path=['pattern', 'phrase'],
        values='freq',
        title='–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑ –ø–æ POS-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º',
        color='freq',
        color_continuous_scale='Blues'
    )
    fig_treemap.update_layout(height=600)
    fig_treemap.show()

print(f"\n–ì–æ—Ç–æ–≤–æ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {len(meaningful_phrases)} –∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑")